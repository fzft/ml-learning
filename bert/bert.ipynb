{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-18T01:54:56.062374Z",
     "start_time": "2023-12-18T01:54:54.376010Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "     sentence_source  label label_notes  \\\n0               gj04      1         NaN   \n1               gj04      1         NaN   \n2               gj04      1         NaN   \n3               gj04      1         NaN   \n4               gj04      1         NaN   \n...              ...    ...         ...   \n8546            ad03      0           *   \n8547            ad03      0           *   \n8548            ad03      1         NaN   \n8549            ad03      1         NaN   \n8550            ad03      1         NaN   \n\n                                               sentence  \n0     Our friends won't buy this analysis, let alone...  \n1     One more pseudo generalization and I'm giving up.  \n2      One more pseudo generalization or I'm giving up.  \n3        The more we study verbs, the crazier they get.  \n4             Day by day the facts are getting murkier.  \n...                                                 ...  \n8546                   Poseidon appears to own a dragon  \n8547                     Digitize is my happiest memory  \n8548                     It is easy to slay the Gorgon.  \n8549       I had the strangest feeling that I knew you.  \n8550                What all did you get for Christmas?  \n\n[8551 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence_source</th>\n      <th>label</th>\n      <th>label_notes</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Our friends won't buy this analysis, let alone...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>One more pseudo generalization and I'm giving up.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>One more pseudo generalization or I'm giving up.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>The more we study verbs, the crazier they get.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>gj04</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>Day by day the facts are getting murkier.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8546</th>\n      <td>ad03</td>\n      <td>0</td>\n      <td>*</td>\n      <td>Poseidon appears to own a dragon</td>\n    </tr>\n    <tr>\n      <th>8547</th>\n      <td>ad03</td>\n      <td>0</td>\n      <td>*</td>\n      <td>Digitize is my happiest memory</td>\n    </tr>\n    <tr>\n      <th>8548</th>\n      <td>ad03</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>It is easy to slay the Gorgon.</td>\n    </tr>\n    <tr>\n      <th>8549</th>\n      <td>ad03</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>I had the strangest feeling that I knew you.</td>\n    </tr>\n    <tr>\n      <th>8550</th>\n      <td>ad03</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>What all did you get for Christmas?</td>\n    </tr>\n  </tbody>\n</table>\n<p>8551 rows Ã— 4 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./cola_public/raw/in_domain_train.tsv', delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
    "\n",
    "df "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T01:54:56.863615Z",
     "start_time": "2023-12-18T01:54:56.064585Z"
    }
   },
   "id": "583866498b33bc52"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "labels = df.label.values\n",
    "sentences = df.sentence.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T01:59:09.950078Z",
     "start_time": "2023-12-18T01:59:08.551145Z"
    }
   },
   "id": "9301a6eb74887ec5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Tokenized:  ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
      "Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]\n"
     ]
    }
   ],
   "source": [
    "print(' Original: ', sentences[0])\n",
    "\n",
    "print('Tokenized: ', tokenizer.tokenize(df.sentence[0]))\n",
    "\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df.sentence[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T01:59:10.654277Z",
     "start_time": "2023-12-18T01:59:10.646995Z"
    }
   },
   "id": "e56cec93e50539f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  Our friends won't buy this analysis, let alone the next one we propose.\n",
      "Token IDs:  [101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "# [SEP] token is a special token that is used to separate two sentences.\n",
    "# [CLS] token is a special token that is used in classification tasks.\n",
    "\n",
    "input_ids = []\n",
    "for sent in sentences:\n",
    "    encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs: ', input_ids[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T02:00:11.243059Z",
     "start_time": "2023-12-18T02:00:09.513992Z"
    }
   },
   "id": "7625a907b2d4ae28"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  47\n"
     ]
    }
   ],
   "source": [
    "print('Max sentence length: ', max([len(sen) for sen in input_ids]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T02:00:55.690511Z",
     "start_time": "2023-12-18T02:00:55.687684Z"
    }
   },
   "id": "940d5c7043f06542"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padding/truncating all sentences to 64 values...\n",
      "\n",
      "Padding token: \"[PAD]\", ID: 0\n",
      "padded_input_ids shape:  torch.Size([8551, 47])\n",
      "Padding ! tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,\n",
      "         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/93/1rzwtwg956b99qtkd3h2j3_40000gn/T/ipykernel_4407/3039977129.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = [torch.tensor(sentence).clone().detach() for sentence in input_ids]\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "MAX_LEN = 64\n",
    "\n",
    "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
    "\n",
    "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "\n",
    "input_ids = [torch.tensor(sentence).clone().detach() for sentence in input_ids]\n",
    "\n",
    "padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "if padded_input_ids.size(1) > MAX_LEN:\n",
    "    padded_input_ids = padded_input_ids[:, :MAX_LEN]\n",
    "    \n",
    "print('padded_input_ids shape: ', padded_input_ids.shape)\n",
    "\n",
    "print('Padding !', padded_input_ids[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T02:53:32.214215Z",
     "start_time": "2023-12-18T02:53:32.083427Z"
    }
   },
   "id": "a8ff22bba4ac280c"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "\n",
    "for sent in padded_input_ids:\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    attention_masks.append(att_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T02:53:38.807929Z",
     "start_time": "2023-12-18T02:53:36.981498Z"
    }
   },
   "id": "308800aec213ef82"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7,695 training samples\n",
      "  856 validation samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, TensorDataset\n",
    "\n",
    "train_idx, val_idx = train_test_split(list(range(len(labels))), test_size=0.2, random_state=42)\n",
    "dataset = TensorDataset(padded_input_ids, torch.tensor(labels), torch.tensor(attention_masks))\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset = Subset(dataset, val_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T02:53:39.914559Z",
     "start_time": "2023-12-18T02:53:39.902883Z"
    }
   },
   "id": "2a13774943972d17"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d589d3bed1a2401c8393b8259279a669"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2, output_attentions=False, output_hidden_states=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T06:28:05.201532Z",
     "start_time": "2023-12-18T06:26:01.909336Z"
    }
   },
   "id": "3e9e466bc11199ce"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-18T06:28:20.626676Z",
     "start_time": "2023-12-18T06:28:20.621662Z"
    }
   },
   "id": "2578c5074e65e70f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52f3b2959c5092d3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
